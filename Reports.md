<h1>Some reports & Manuscripts</h1>

[Back to home page](README.md)

## Reading manuscripts on EM Convergence Rate

**[EM Convergence Rate](EM Convergence Rate.pdf)**\
Ziyi Song, Oct 2020, Ann Arbor, MI

This manuscript introduces Theorem 1 & Theorem 2 in paper [R. Dwivedi, N. Ho & K. Khamaru (2020)](https://arxiv.org/abs/1810.00828). The manuscript mainly shows that,
  - given an unbalanced Gaussian mixture model with two components, population EM Algo converges geometrically to the ture parameter wherever the initialization starts, and sample EM Algo converges geometrically to a ball with radius around the true parameter;
  - given a balanced Gaussian mixture model with two components, population EM still converges to the true parameter from arbitrary initialization, but the convergence gets exponentially slower when the iteration close to the true parameter.




## Reading reports on Dirichlet Process prior

**[DP Mixture inference derivations: an example](Ziyi DPM_infer_deri_example.pdf)**\
Ziyi Song, June 2020, Ann Arbor, MI

In this report, we discuss issues in Gaussian mixture model using data distributions derived as normal mixtures in the framework of Dirichlet processes. Besides dealing with these issues, as a natural by-product, we develop approaches to inference about the number of components and modes in a population distribution. This report follows Escobar and West (1995).


**[Dirichlet Process Report](Ziyi Dirichlet Process Report.pdf)**\
Ziyi Song, May 2020, Ann Arbor, MI
               
This report is comprised of what I think is most fundamental in Bayesian Nonparametric for novices. The report goes in a sequence of topics: Dirichlet Processes, Dirichlet Process Mixtures, Markov Chain Sampling for Dirichlet Process Mixture models, Hierarchical Dirichlet Processes, applications on autonomous multi-vehicle interaction sce- narios modeling.


## Course project reports

**[An adventure in Semi-supervised learning](stats 601 project report.pdf), [slides](Exploring_Semi_supervised_learning.pdf), STATS 601**\
Trong Dat Do & Ziyi Song, April 2020, Ann Arbor, MI

 In this project, we study some semi-supervised learning algorithms and their connection to those that we learn on class. We focus on two questions:
  - How can we use unlabeled data to improve classification quality?
  - How can the limited labeled data help with clustering, especially when some labels not appeared in the labeled data but hidden in the unlabeled part, with a Semi Dirichlet Process Mixture model?


**[Adaptive Monte-Carlo Optimization](Biostat 615 project report.pdf), [slides](Adaptive Monte Carlo.pdf), BIOSTATS 615**\
